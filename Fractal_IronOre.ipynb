{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fractal_IronOre",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "I5ERs_ewhO9k",
        "jhSfj59AhfhM",
        "uzAesX5stqL_",
        "C5Sz2yscnn4F",
        "2eUiNT8Anqay",
        "c1lPEO8ztvZk",
        "5QldRWQwFF5_",
        "XxqUXlHhO7b_",
        "GfR7ZhTive9B",
        "4XrKAz5Ax2L4",
        "GqusSE02yw8G",
        "lK-ngssUYrSO",
        "uFYRSZxy64SY",
        "7DUJRnm9HmIT",
        "0QPyOOJOd5Vh",
        "UmwRw-lcnAt8",
        "KazlAiK9nFr3",
        "hofxbO3-cRab",
        "WjhXl2F82aDT",
        "MoVXnsFQ8kGL",
        "lHvTt7h58oST",
        "rfKXkjAF8qwF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/richard-cartwright/personal/blob/master/Fractal_IronOre.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "I5ERs_ewhO9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SUMMARY\n",
        "\n",
        "Q1. How much iron ore did Brazil export Jan-May 2017? \n",
        "- = ~125million DWT\n",
        "\n",
        "Q2. Proportion of ships coming from South Africa, Indian Ocean, North Atlantic Ocean \n",
        "- = ~0.6\n",
        "\n",
        "Q3. What features of the Capesize position history are helpful in predicting the behaviour of the index?\n",
        "- If all ships far from shore = no ships available that day = low supply = higher price\n",
        "- If all ships far from their destination = no ships available in coming days = low supply = high price\n",
        "- If all ships low under water (high draft) = they're already carrying lots of weight = limited spare capacity = low supply = high price\n"
      ]
    },
    {
      "metadata": {
        "id": "uzAesX5stqL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PACKAGES"
      ]
    },
    {
      "metadata": {
        "id": "C5Sz2yscnn4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ]
    },
    {
      "metadata": {
        "id": "kZkzezqanfoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# to visualise ROC curve\n",
        "# !pip install scikit-plot\n",
        "\n",
        "# for read_excel\n",
        "!pip install xlrd\n",
        "\n",
        "# for extracting country from lat-lon\n",
        "!pip install reverse_geocode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eUiNT8Anqay",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "mQmlSdyU4PHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Basic imports, including ML libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import xlrd\n",
        "\n",
        "import pprint\n",
        "%matplotlib inline\n",
        "\n",
        "# Setting plotting styles\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set_style('white')\n",
        "\n",
        "# Displays all cell's output, not just last output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# Sklearn\n",
        "# import scikitplot as skplt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler,PolynomialFeatures, Imputer\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, recall_score, brier_score_loss, f1_score\n",
        "\n",
        "# Tensorflow & Keras\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1lPEO8ztvZk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ENVIRON SET-UP\n"
      ]
    },
    {
      "metadata": {
        "id": "qdFhppw3IARx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add GDrive to Colab environment\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create path for data\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Personal/Fractal/Data/'\n",
        "\n",
        "# View files in folder\n",
        "!ls '/content/drive/My Drive/Colab Notebooks/Personal/Fractal/Data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VtG4oe4xzsKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract data\n",
        "\n",
        "fleet_reg_df = pd.read_csv(path+'fleet_reg.csv',\n",
        "                           index_col='imo',\n",
        "                           parse_dates=['built','last_update','launch_date','order_date','year','broken_up','created_at','updated_at'])\\\n",
        "                  .sort_index()\n",
        "\n",
        "vessel_position_df = pd.read_csv(path+'vsl_pos_170101_170531.csv',\n",
        "                                 index_col=['imo','date'],\n",
        "                                 parse_dates=['date','eta'])\\\n",
        "                        .sort_index()\n",
        "\n",
        "capesize_index_df = pd.read_csv(path+'bci_170101_170531.csv',\n",
        "                                index_col='date',\n",
        "                                parse_dates=['date'])\\\n",
        "                        .sort_index()\\\n",
        "                        .drop(columns=['name']) # Always 'BCI 5TC'\n",
        "\n",
        "portlog_df = pd.read_excel(path+'portlog.xlsx',\n",
        "                           index_col='name')\\\n",
        "                  .sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BljYQ-wQG_vv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# INITIAL EDA"
      ]
    },
    {
      "metadata": {
        "id": "dfYDCPmLGY2D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## fleet_reg_df\n",
        "\n",
        "*A fleet register which identifies dry bulk cargo ships (name, deadweight = carrying capacity, age etc.) by their unique IMO (International Maritime Organization) number.*"
      ]
    },
    {
      "metadata": {
        "id": "-hkwhS2X6z_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fleet_reg_df\n",
        "\n",
        "# fleet_reg_df.info()\n",
        "\n",
        "# x=0\n",
        "# num_cols = 8\n",
        "# while x<=38:\n",
        "#   fleet_reg_df.iloc[:,x:x+num_cols].head(2)\n",
        "#   x+=num_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xAq_yhnVGeLR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## vessel_position_df\n",
        "\n",
        "*Vessel position reports for Jan-May 2017. This is AIS (Automatic Identification System) satellite data. Column meanings: IMO, date of reading, latitude of ship, longitude of ship, speed (in knots), draft (in meters - the depth of the ship under water), indicated ETA, indicated destination (a free text field populated by the crew), indicated vessel status.*"
      ]
    },
    {
      "metadata": {
        "id": "A3uYQdusBv_Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# vessel_position_df\n",
        "\n",
        "# vessel_position_df.head(2)\n",
        "# vessel_position_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R3phW8ESGiMI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## capesize_index_df\n",
        "\n",
        "*The Baltic Capesize Timecharter Index: this represents the average daily hire rate which Capesize vessels (deadweight > 120,000t) earned (in $/day) when agreeing that day to a new round-trip voyage from a given discharge port, to a load port, and back to a discharge port. This is the index against which Capesize freight futures settle.*"
      ]
    },
    {
      "metadata": {
        "id": "p9jJg_i_EeAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# capesize_index_df\n",
        "\n",
        "# capesize_index_df.head(2)\n",
        "# capesize_index_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U_lLv18gGlD7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## portlog_df\n",
        "\n",
        "*A portlog which specifies the geographical coordinates (latitude minmax, longitude minmax) of areas containing some of the major world ports.*"
      ]
    },
    {
      "metadata": {
        "id": "BxAavO2oEh4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# portlog_df\n",
        "\n",
        "# portlog_df.head(2)\n",
        "# portlog_df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5QldRWQwFF5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q1. How much iron ore did Brazil export Jan-May 2017? = ~125million DWT\n",
        "*This is comprised of the cumulative deadweight of all large vessels (with a dwt > 60,000) that entered and then exited the following set of ports during the period: Ponta da Madeira, Tubarao, Ponta Ubu, Porto Acu, ItaguaiGuaiba.*"
      ]
    },
    {
      "metadata": {
        "id": "XxqUXlHhO7b_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clean 'destination' data"
      ]
    },
    {
      "metadata": {
        "id": "-hecidmgOeQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Clean 'destination' column: standardise name for the ports I care about\n",
        "\n",
        "# Make 'destination' column: 1) string dtype; 2) lower case for better regex\n",
        "vessel_position_df['destination'] = vessel_position_df['destination'].astype(str).apply(lambda dest: dest.lower())\n",
        "\n",
        "# List of ports I care about\n",
        "ports = ['madeira','tubarao','ubu','acu','itaguai','guaiba']\n",
        "\n",
        "# List of unique unclean 'destination' for all vessel_position_df\n",
        "destinations_raw = vessel_position_df['destination'].unique()\n",
        "\n",
        "destinations_ports = {}\n",
        "# Create dictionary of all raw unclean 'destination' for each port\n",
        "for port in ports:\n",
        "  destinations_ports[port] = [dest for dest in destinations_raw if port in dest]\n",
        "\n",
        "# Make manual tweaks to extract just the names relevant to the port\n",
        "destinations_ports['ubu'] = [dest for dest in destinations_ports['ubu'] \n",
        "                                  if 'aubu' not in dest \n",
        "                                  and 'lubu' not in dest\n",
        "                                  ]\n",
        "destinations_ports['acu'] = [dest for dest in destinations_ports['acu'] \n",
        "                                  if 'macu' not in dest \n",
        "                                  and 'jacu' not in dest\n",
        "                                  and 'yacu' not in dest\n",
        "                                  and 'sin_acu' not in dest\n",
        "                                  and 'iacu' not in dest\n",
        "                                  ]\n",
        "\n",
        "# Display unclean port names\n",
        "# for port in ports:\n",
        "#   print(port,'\\n')\n",
        "#   destinations_ports[port]\n",
        "#   print('\\n')\n",
        "\n",
        "# Replace unclean port names with cleaned\n",
        "for port in ports:\n",
        "  vessel_position_df['destination'] = vessel_position_df['destination'].replace(destinations_ports[port],port)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GfR7ZhTive9B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extract export movements data"
      ]
    },
    {
      "metadata": {
        "id": "4vEXAocia6Lt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create df of the previous and current destination of each vessel position\n",
        "export_moment_df = pd.concat(axis=1, \n",
        "                             objs=[vessel_position_df['destination'].shift(1),\n",
        "                                  vessel_position_df['destination']])\n",
        "export_moment_df.columns = ['previous_destination','new_destination']\n",
        "\n",
        "# ----------\n",
        "# ISSUE: because of .shift(), last destination of the previous imo erroneously becomes the 'previous_destination' for the next imo\n",
        "# Therefore refill these first 'previous_destination' with 'Unknown'\n",
        "\n",
        "# Create series of first dates\n",
        "first_dates = export_moment_df.reset_index().groupby('imo',as_index=False)['date'].first()\n",
        "\n",
        "# Set previous destination of first dates as 'Unknown'\n",
        "first_dates['previous_destination'] = 'Unknown'\n",
        "first_dates.set_index(['imo','date'],inplace=True)\n",
        "\n",
        "# Set first previous_destination as 'Unknown', instead of just the erroneous shifted destination\n",
        "export_moment_df.loc[first_dates.index,'previous_destination'] = first_dates['previous_destination']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQtHH5sDdLRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "53dc5235-a291-4636-aa45-6666ba16bfb7"
      },
      "cell_type": "code",
      "source": [
        "# Extract only those moments where the previous_destination is a Brazilian IO port,\n",
        "# and the new destination is NOT one of those ports (therefore exported out of Brazil)\n",
        "export_moment_df = export_moment_df[\n",
        "    (export_moment_df['previous_destination'].isin(ports)) \n",
        "    & (~export_moment_df['new_destination'].isin(ports))\n",
        "]\n",
        "\n",
        "# Join on 'dwt' from fleet reference table\n",
        "export_moment_df = pd.merge(export_moment_df.reset_index(),\n",
        "                            fleet_reg_df[['dwt']],\n",
        "                            how='left',\n",
        "                            left_on='imo',\n",
        "                            right_index=True)\\\n",
        "                         .set_index(['imo','date'])\\\n",
        "                         .sort_index()\n",
        "\n",
        "export_moment_df.head(2)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>previous_destination</th>\n",
              "      <th>new_destination</th>\n",
              "      <th>dwt</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imo</th>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8420804</th>\n",
              "      <th>2017-05-02 00:11:35</th>\n",
              "      <td>tubarao</td>\n",
              "      <td>br pdm</td>\n",
              "      <td>364767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8800286</th>\n",
              "      <th>2017-05-14 23:59:31</th>\n",
              "      <td>tubarao</td>\n",
              "      <td>kwang yang</td>\n",
              "      <td>290160</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            previous_destination new_destination     dwt\n",
              "imo     date                                                            \n",
              "8420804 2017-05-02 00:11:35              tubarao          br pdm  364767\n",
              "8800286 2017-05-14 23:59:31              tubarao      kwang yang  290160"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "4XrKAz5Ax2L4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exports = ~125million DWT"
      ]
    },
    {
      "metadata": {
        "id": "2ijgUYvotbd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "748074ef-bc0a-4a7d-bce0-6b2b53f04bcb"
      },
      "cell_type": "code",
      "source": [
        "# Restrict to large (dwt>60000) and only Jan to May 2017\n",
        "large_janToMay = export_moment_df.loc[(slice(None),slice('2017-01','2017-05')),:][export_moment_df.dwt>60000]\n",
        "\n",
        "print('The cumulative DWT of all large vessels who exported from Brazilian Iron Ore ports between Jan & May 2017 is:',\n",
        "      large_janToMay['dwt'].sum())"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cumulative DWT of all large vessels who exported from Brazilian Iron Ore ports between Jan & May 2017 is: 125335892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GqusSE02yw8G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q2. Proportion of ships coming from South Africa, Indian Ocean, North Atlantic Ocean = ~0.6\n",
        "\n",
        "*Those are the two common areas for ships that come and load iron ore in Brazil.*"
      ]
    },
    {
      "metadata": {
        "id": "5d3iYxvhF84C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c1db07a6-fc58-4c6e-bc7b-0197e763cc6c"
      },
      "cell_type": "code",
      "source": [
        "# reverse_geocode package for extracting country from lat-lon\n",
        "\n",
        "import reverse_geocode\n",
        "\n",
        "# Example\n",
        "coordinates = ((-19.900000, 148.1000), (31.76, 35.21)) #tuple\n",
        "reverse_geocode.search(coordinates)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'city': 'Bowen', 'country': 'Australia', 'country_code': 'AU'},\n",
              " {'city': 'Jerusalem', 'country': 'Israel', 'country_code': 'IL'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "metadata": {
        "id": "mVX6Ld8NHe25",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create df of 'previous_destination', 'new_destination' & lat-lon for each vessel position\n",
        "departure_moment_df = pd.concat(axis=1, \n",
        "                             objs=[vessel_position_df['destination'].shift(1),\n",
        "                                  vessel_position_df[['destination','lat','lon']]])\n",
        "departure_moment_df.columns = ['previous_destination','new_destination','departure_lat','departure_lon']\n",
        "\n",
        "# Extract only those moments where the previous_destination is NOT a Brazilian IO port,\n",
        "# and the new_destination IS a one of those ports\n",
        "departure_moment_df = departure_moment_df[\n",
        "    (~departure_moment_df['previous_destination'].isin(ports)) \n",
        "    & (departure_moment_df['new_destination'].isin(ports))\n",
        "]\n",
        "\n",
        "# Create departure_country from departure lat-lon\n",
        "departure_moment_df['departure_country'] = departure_moment_df.apply(\n",
        "    axis=1,\n",
        "    func=lambda row: reverse_geocode.search(\n",
        "        ((row['departure_lat'],row['departure_lon']),)\n",
        "    )[0]['country']\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGJzfnR6OCeY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# countries from 'departure_country' in: South Africa, Indian Ocean, North Atlantic Ocean\n",
        "\n",
        "selected_countries = [\n",
        "    'Belgium',\n",
        "    'Canada',\n",
        "    'Cape Verde',\n",
        "    'Cocos (Keeling) Islands',\n",
        "    'Comoros',\n",
        "    'Finland',\n",
        "    'France',\n",
        "    'Germany',\n",
        "    'Gibraltar',\n",
        "    'India',\n",
        "    'Indonesia',\n",
        "    'Kenya',\n",
        "    'Latvia',\n",
        "    'Madagascar',\n",
        "    'Maldives',\n",
        "    'Mauritius',\n",
        "    'Morocco',\n",
        "    'Mozambique',\n",
        "    'Namibia',\n",
        "    'Netherlands',\n",
        "    'Portugal',\n",
        "    'Saint Helena',\n",
        "    'South Africa',\n",
        "    'Spain',\n",
        "    'Sri Lanka',\n",
        "    'Sweden',\n",
        "    'United Kingdom',\n",
        "    'United States']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTd53KkAXs2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a29aa6c-a2e7-424f-e70c-44793c4454d2"
      },
      "cell_type": "code",
      "source": [
        "print('Proportion of ships travelling to Brazilian IO ports coming from South Africa, Indian Ocean, North Atlantic Ocean: ',\n",
        "      round(sum(departure_moment_df['departure_country'].isin(selected_countries))\n",
        "            / len(departure_moment_df),\n",
        "            3))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proportion of ships travelling to Brazilian IO ports coming from South Africa, Indian Ocean, North Atlantic Ocean:  0.581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lK-ngssUYrSO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q3. What features of the Capesize position history are helpful in predicting the behaviour of the index? \n",
        "\n",
        "Early answer:\n",
        "- draft\n",
        "- lat & lon"
      ]
    },
    {
      "metadata": {
        "id": "uFYRSZxy64SY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Supply-Demand of DWT\n",
        "\n",
        "Hypothesis: as available supply of DWT **increases**, price **decreases**\n",
        "\n",
        "Capesize is >120k but will be affected by supply of ships <=120k\n",
        "\n",
        "Dynamics:\n",
        "- This price is forward looking: \"agreeing that day to a new round-trip voyage from a given discharge port\"\n",
        "- Therefore, if a ship space **seller** expects a higher price tomorrow, the space seller will not agree to the (lower) price today and instead will wait for tomorrow\n",
        "- Therefore, the ship space **buyer** will bid a higher price today to tempt the seller to accept\n",
        "- Therefore, tomorrow's price feeds positively back into today's price: if tomorrow's price will be higher, then this will drag today's price higher\n",
        "- Tomorrow's price is dictated by **supply of DWT**: if tomorrow's DWT is lower, tomorrow's price will be higher, pushing today's price higher"
      ]
    },
    {
      "metadata": {
        "id": "lE1pAyISlztl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How to measure supply of DWT?\n",
        "\n",
        "Intuitively:\n",
        "- If all ships **far from shore** = no ships available that day = low supply = higher price\n",
        "- If all ships **far from their destination** = no ships available in coming days = low supply = high price\n",
        "- If all ships **low under water (high draft)** = they're already carrying lots of weight = limited spare capacity = low supply = high price\n",
        "\n",
        "(*Features are aggregated across all ships for each day*)\n",
        "\n",
        "Features: \n",
        "- **draft** mean,median,std: if average draft larger, less spare capacity\n",
        "- **hours_until_ETA** mean,median,std: if longer until destination, less capacity at the shore\n",
        "- **status** dummies: whether more ships are Anchored vs Moored vs Sailing will dictate how much spare capacity there is\n",
        "- **speed** mean,median,std: proxies activity of ship. High speed if sailing, low speed if moored\n",
        "\n",
        "Features I'm unsure about but may have predictive power:\n",
        "- **number of unique**: this is number of ships each day which give out vessel_position\n",
        "- **lat & lon**: this proxies positions of ship so can say how far ships are from shores"
      ]
    },
    {
      "metadata": {
        "id": "7DUJRnm9HmIT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create aggregate daily features for vessel positions"
      ]
    },
    {
      "metadata": {
        "id": "Hr5QdKZkqQYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "67786b5d-3a82-4a7e-a1ff-703a23178241"
      },
      "cell_type": "code",
      "source": [
        "vessel_position_df.head(2)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>speed</th>\n",
              "      <th>draft</th>\n",
              "      <th>eta</th>\n",
              "      <th>destination</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imo</th>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">7105495</th>\n",
              "      <th>2017-01-01 00:09:29</th>\n",
              "      <td>46.6974</td>\n",
              "      <td>-92.0187</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>8.5</td>\n",
              "      <td>2017-01-03 05:11:00</td>\n",
              "      <td>burns hbr</td>\n",
              "      <td>Moored</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-01 22:54:29</th>\n",
              "      <td>47.2661</td>\n",
              "      <td>-86.5826</td>\n",
              "      <td>13.0346</td>\n",
              "      <td>8.5</td>\n",
              "      <td>2017-01-03 15:11:00</td>\n",
              "      <td>burns hbr</td>\n",
              "      <td>Under way using engine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 lat      lon    speed  draft  \\\n",
              "imo     date                                                    \n",
              "7105495 2017-01-01 00:09:29  46.6974 -92.0187   0.0000    8.5   \n",
              "        2017-01-01 22:54:29  47.2661 -86.5826  13.0346    8.5   \n",
              "\n",
              "                                            eta destination  \\\n",
              "imo     date                                                  \n",
              "7105495 2017-01-01 00:09:29 2017-01-03 05:11:00   burns hbr   \n",
              "        2017-01-01 22:54:29 2017-01-03 15:11:00   burns hbr   \n",
              "\n",
              "                                             status  \n",
              "imo     date                                         \n",
              "7105495 2017-01-01 00:09:29                  Moored  \n",
              "        2017-01-01 22:54:29  Under way using engine  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "RVOjwCGXRjQv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Derive 'status' dummies & 'hours_until_ETA'\n",
        "\n",
        "# Dummies for categories of 'status'\n",
        "vessel_position_features_df = pd.get_dummies(vessel_position_df.reset_index(),\n",
        "                                                   columns=['status'],\n",
        "                                                   dummy_na=True)\n",
        "\n",
        "# Derive 'hours_until_ETA'\n",
        "vessel_position_features_df['hours_until_ETA'] = (vessel_position_features_df['eta'] \n",
        "                                                  - vessel_position_features_df['date']).dt.seconds / 3600\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dO0EPvn5Vzy3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create aggregate daily features\n",
        "\n",
        "daily_features_dict = {}\n",
        "\n",
        "# Extract series of the datetimes\n",
        "dates = vessel_position_features_df['date'].dt.date\n",
        "\n",
        "# Number of unique imo each day \n",
        "daily_features_dict['nunique'] = vessel_position_features_df.groupby(dates)[['imo']].nunique().rename(columns={'imo':'nunique'})\n",
        "\n",
        "# distribution variables each day for: lat,lon,speed,draft,hours_until_ETA\n",
        "daily_features_dict['numerical'] = vessel_position_features_df.groupby(dates)[\n",
        "    ['lat','lon','speed','draft','hours_until_ETA']].agg(['mean','median','std'])\n",
        "\n",
        "# Get mean each day of status dummies\n",
        "daily_features_dict['status_dummies'] = vessel_position_features_df.groupby(dates)[\n",
        "    [col for col in vessel_position_features_df.columns if col.startswith('status')]].mean()\n",
        "\n",
        "# Create new df\n",
        "position_daily_features_df = pd.concat(axis=1,\n",
        "                                       objs=[daily_features_dict['nunique'],\n",
        "                                             daily_features_dict['numerical'],\n",
        "                                             daily_features_dict['status_dummies']])\n",
        "# Datetimeindex\n",
        "position_daily_features_df.index = pd.to_datetime(position_daily_features_df.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NlEXgTcUYq6N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# EDA\n",
        "\n",
        "# position_daily_features_df.info()\n",
        "\n",
        "# # viz each variable over time\n",
        "# for col in position_daily_features_df.columns:\n",
        "#   position_daily_features_df[col].iloc[0:-1].plot();\n",
        "#   plt.title(col);\n",
        "#   plt.figure();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0QPyOOJOd5Vh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Early model on basic data\n",
        "\n",
        "- Just using at-time variables, no moving averages or pct changes\n",
        "- The motivation is to get a baseline accuracy"
      ]
    },
    {
      "metadata": {
        "id": "hXFG8BIifJq3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create different types of targets: regression & classification\n",
        "\n",
        "basic_targets = capesize_index_df.shift(-1).rename(columns={'value':'next_price'})\n",
        "\n",
        "# Regression target\n",
        "basic_targets['next_pct_change'] = capesize_index_df.pct_change().shift(-1)\n",
        "\n",
        "# Classification target\n",
        "basic_targets['next_price_higher'] = (capesize_index_df.pct_change().shift(-1) > 0)\n",
        "basic_targets = basic_targets.dropna()\n",
        "\n",
        "# Use only data when I have a target\n",
        "basic_model_data = position_daily_features_df.reindex(index=basic_targets.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UmwRw-lcnAt8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ]
    },
    {
      "metadata": {
        "id": "9JulfTekg9n7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "29b1b737-02ea-4b58-d856-76ba97f2635c"
      },
      "cell_type": "code",
      "source": [
        "# Classification - up or down\n",
        "\n",
        "# Data train-test split\n",
        "y = basic_targets['next_price_higher']\n",
        "X = basic_model_data\n",
        "\n",
        "train_threshold = round(0.7*len(basic_model_data))\n",
        "\n",
        "X_train = X.iloc[:train_threshold]\n",
        "y_train = y[:train_threshold]\n",
        "X_test = X.iloc[train_threshold:]\n",
        "y_test = y[train_threshold:]\n",
        "\n",
        "# ---\n",
        "# Models\n",
        "forest = RandomForestClassifier(random_state=42,\n",
        "                                n_estimators=100,\n",
        "                                max_depth=2)\n",
        "forest.fit(X_train,y_train)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train,y_train)\n",
        "\n",
        "print('\\n')\n",
        "print('Score by chance = 0.5')\n",
        "\n",
        "print('\\n')\n",
        "print('Basic RandomForest Classificationtrain score:',forest.score(X_train,y_train))\n",
        "print('Basic RandomForest Classification test score:',forest.score(X_test,y_test))\n",
        "\n",
        "print('\\n')\n",
        "print('Basic LogReg Classification train score:',logreg.score(X_train,y_train))\n",
        "print('Basic LogReg Classification test score:',logreg.score(X_test,y_test))\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = pd.DataFrame(forest.feature_importances_,\n",
        "                                   index = X_train.columns,\n",
        "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
        "\n",
        "# Print 10 most important features\n",
        "print('\\n',feature_importances.head(10))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
              "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
              "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Score by chance = 0.5\n",
            "\n",
            "\n",
            "Basic RandomForest Classificationtrain score: 0.8873239436619719\n",
            "Basic RandomForest Classification test score: 0.5806451612903226\n",
            "\n",
            "\n",
            "Basic LogReg Classification train score: 0.7746478873239436\n",
            "Basic LogReg Classification test score: 0.6774193548387096\n",
            "\n",
            "                                importance\n",
            "(draft, mean)                    0.102725\n",
            "status_nan                       0.097739\n",
            "status_Under way using engine    0.087403\n",
            "(lon, mean)                      0.073068\n",
            "status_Anchored                  0.071703\n",
            "nunique                          0.065296\n",
            "(lat, std)                       0.059790\n",
            "(hours_until_ETA, median)        0.053535\n",
            "status_Moored                    0.050951\n",
            "(speed, mean)                    0.047769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KazlAiK9nFr3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Regression"
      ]
    },
    {
      "metadata": {
        "id": "V785em50naIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "125c8c39-131f-4eb3-b4ed-937accf8e11c"
      },
      "cell_type": "code",
      "source": [
        "# Regression - quantify percentage price change\n",
        "\n",
        "# Data train-test split\n",
        "y = basic_targets['next_pct_change']\n",
        "X = basic_model_data\n",
        "\n",
        "train_threshold = round(0.7*len(basic_model_data))\n",
        "\n",
        "X_train = X.iloc[:train_threshold]\n",
        "y_train = y[:train_threshold]\n",
        "X_test = X.iloc[train_threshold:]\n",
        "y_test = y[train_threshold:]\n",
        "\n",
        "# ---\n",
        "# Models\n",
        "forest = RandomForestRegressor(random_state=42,\n",
        "                               n_estimators=100,\n",
        "                               max_depth=2)\n",
        "forest.fit(X_train,y_train)\n",
        "\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train,y_train)\n",
        "\n",
        "print('\\n')\n",
        "print('Score by zero prediction = 0')\n",
        "\n",
        "print('\\n')\n",
        "print('Basic RandomForest Regression train score:',forest.score(X_train,y_train))\n",
        "print('Basic RandomForest Regression test score:',forest.score(X_test,y_test))\n",
        "\n",
        "print('\\n')\n",
        "print('Basic LinReg Regression train score:',linreg.score(X_train,y_train))\n",
        "print('Basic LinReg Regression test score:',linreg.score(X_test,y_test))\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = pd.DataFrame(forest.feature_importances_,\n",
        "                                   index = X_train.columns,\n",
        "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
        "\n",
        "# Print 10 most important features\n",
        "print('\\n',feature_importances.head(10))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
              "           max_features='auto', max_leaf_nodes=None,\n",
              "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "           min_samples_leaf=1, min_samples_split=2,\n",
              "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
              "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Score by zero prediction = 0\n",
            "\n",
            "\n",
            "Basic RandomForest Regression train score: 0.5530932840451284\n",
            "Basic RandomForest Regression test score: -0.3678249600751802\n",
            "\n",
            "\n",
            "Basic LinReg Regression train score: 0.5117790499048771\n",
            "Basic LinReg Regression test score: -16.282206944099705\n",
            "\n",
            "                                importance\n",
            "(draft, mean)                    0.202861\n",
            "status_Under way using engine    0.167726\n",
            "(lon, mean)                      0.133484\n",
            "nunique                          0.076158\n",
            "status_Anchored                  0.065567\n",
            "(hours_until_ETA, median)        0.057347\n",
            "status_Moored                    0.036879\n",
            "status_nan                       0.034002\n",
            "(speed, mean)                    0.028698\n",
            "(draft, median)                  0.025647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hofxbO3-cRab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model data\n",
        "\n",
        "Using:\n",
        "- unweighted **rolling means**: 2,5,10 day\n",
        "- **percent changes**: 1,4,9 day"
      ]
    },
    {
      "metadata": {
        "id": "BTDOcGbV3dtk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MAs & pct_changes"
      ]
    },
    {
      "metadata": {
        "id": "jfpuceHhukUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Capesize MA & pct_change to capture autoregressive tendencies\n",
        "\n",
        "# Capesize index interpolated to include weekends\n",
        "interpolated_capesize_index = capesize_index_df.reindex(position_daily_features_df.index).interpolate()\n",
        "\n",
        "# Capesize MAs & pct_changes\n",
        "capesize_features_df = pd.concat(axis=1,\n",
        "                                 objs=[interpolated_capesize_index.pct_change(1),\n",
        "                                       interpolated_capesize_index.pct_change(4),\n",
        "                                       interpolated_capesize_index.pct_change(9),\n",
        "                                       interpolated_capesize_index.rolling(2).mean(),\n",
        "                                       interpolated_capesize_index.rolling(5).mean(),\n",
        "                                       interpolated_capesize_index.rolling(10).mean()])\n",
        "capesize_features_df.columns = ['capesize_'+x for x in ['pct1','pct4','pct9','ma2','ma5','ma10']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g8RKj2YdrzaX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MAs and pct_changes for all model features\n",
        "\n",
        "# ---\n",
        "# pct_changes\n",
        "\n",
        "# Tweak zero values to avoid infinities with pct_change\n",
        "position_daily_features_df.replace(0,0.000001,inplace=True)\n",
        "\n",
        "pct_1d = position_daily_features_df.pct_change(1)\n",
        "pct_1d.columns = ['pct1_'+str(col) for col in pct_1d.columns]\n",
        "\n",
        "pct_4d = position_daily_features_df.pct_change(4)\n",
        "pct_4d.columns = ['pct4_'+str(col) for col in pct_4d.columns]\n",
        "\n",
        "pct_9d = position_daily_features_df.pct_change(9)\n",
        "pct_9d.columns = ['pct9_'+str(col) for col in pct_9d.columns]\n",
        "\n",
        "# ---\n",
        "# MAs\n",
        "ma_2d = position_daily_features_df.rolling(2).mean()\n",
        "ma_2d.columns = ['ma2_'+str(col) for col in ma_2d.columns]\n",
        "\n",
        "ma_5d = position_daily_features_df.rolling(5).mean()\n",
        "ma_5d.columns = ['ma5_'+str(col) for col in ma_5d.columns]\n",
        "\n",
        "ma_10d = position_daily_features_df.rolling(10).mean()\n",
        "ma_10d.columns = ['ma10_'+str(col) for col in ma_10d.columns]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xa-mgUV0t7hM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model data contains all MAs and pct_changes\n",
        "model_df = pd.concat(axis=1,\n",
        "                     objs=[capesize_features_df,\n",
        "                           position_daily_features_df,\n",
        "                           pct_1d,\n",
        "                           pct_4d,\n",
        "                           pct_9d,\n",
        "                           ma_2d,\n",
        "                           ma_5d,\n",
        "                           ma_10d])\n",
        "\n",
        "# NaNs caused by lag from MAs & pct_changes - therefore can't interpolate\n",
        "# Fill NaN with *median* - to dampen signal but avoid distortion\n",
        "model_df = model_df.fillna(value=model_df.median())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aR9CvDZkxbG9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Only include data with an associated target\n",
        "X_data = model_df.reindex(index=basic_targets.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "F7_nG71P2VJ4"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "00fab0f7-2893-4b4b-a1dd-2157d0d50d0f",
        "id": "EX_hpauz2VJ8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "cell_type": "code",
      "source": [
        "# Classification - up or down\n",
        "y_target = basic_targets['next_price_higher']\n",
        "\n",
        "# Data train-test split\n",
        "train_threshold = round(0.7*len(basic_model_data))\n",
        "\n",
        "X_train = X_data.iloc[:train_threshold]\n",
        "y_train = y_target[:train_threshold]\n",
        "\n",
        "X_test = X_data.iloc[train_threshold:]\n",
        "y_test = y_target[train_threshold:]\n",
        "\n",
        "# ---\n",
        "# Models\n",
        "forest = RandomForestClassifier(random_state=42,\n",
        "                                n_estimators=100,\n",
        "                                max_depth=2)\n",
        "forest.fit(X_train,y_train)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train,y_train)\n",
        "\n",
        "print('\\n')\n",
        "print('Basic RandomForest Classificationtrain score:',forest.score(X_train,y_train))\n",
        "print('Basic RandomForest Classification test score:',forest.score(X_test,y_test))\n",
        "\n",
        "print('\\n')\n",
        "print('Basic LogReg Classification train score:',logreg.score(X_train,y_train))\n",
        "print('Basic LogReg Classification test score:',logreg.score(X_test,y_test))\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = pd.DataFrame(forest.feature_importances_,\n",
        "                                   index = X_train.columns,\n",
        "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
        "\n",
        "# Print 10 most important features\n",
        "print('\\n',feature_importances.head(10))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
              "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
              "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Basic RandomForest Classificationtrain score: 0.971830985915493\n",
            "Basic RandomForest Classification test score: 0.5806451612903226\n",
            "\n",
            "\n",
            "Basic LogReg Classification train score: 0.9859154929577465\n",
            "Basic LogReg Classification test score: 0.3225806451612903\n",
            "\n",
            "                         importance\n",
            "capesize_pct1             0.066114\n",
            "ma5_('lon', 'median')     0.046237\n",
            "ma10_('lon', 'mean')      0.041038\n",
            "ma10_('lon', 'median')    0.034937\n",
            "ma5_('draft', 'mean')     0.030897\n",
            "pct9_('draft', 'std')     0.027329\n",
            "nunique                   0.026596\n",
            "ma5_('lon', 'mean')       0.026387\n",
            "ma5_('speed', 'mean')     0.023495\n",
            "ma10_('draft', 'mean')    0.020926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CJX9PTJr2Z_T"
      },
      "cell_type": "markdown",
      "source": [
        "### Regression"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "cf1b0479-f2df-4b5f-b602-ccd56a974f7b",
        "id": "VhvM5Nq42Z_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression - quantify percentage price change\n",
        "y = basic_targets['next_pct_change']\n",
        "\n",
        "# Data train-test split\n",
        "train_threshold = round(0.7*len(basic_model_data))\n",
        "\n",
        "X_train = X_data.iloc[:train_threshold]\n",
        "y_train = y_target[:train_threshold]\n",
        "\n",
        "X_test = X_data.iloc[train_threshold:]\n",
        "y_test = y_target[train_threshold:]\n",
        "\n",
        "\n",
        "forest = RandomForestRegressor(random_state=42,\n",
        "                               n_estimators=100,\n",
        "                               max_depth=2)\n",
        "forest.fit(X_train,y_train)\n",
        "\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train,y_train)\n",
        "\n",
        "print('\\n')\n",
        "print('Basic RandomForest Regression train score:',forest.score(X_train,y_train))\n",
        "print('Basic RandomForest Regression test score:',forest.score(X_test,y_test))\n",
        "\n",
        "print('\\n')\n",
        "print('Basic LinReg Regression train score:',linreg.score(X_train,y_train))\n",
        "print('Basic LinReg Regression test score:',linreg.score(X_test,y_test))\n",
        "\n",
        "# Feature importances\n",
        "feature_importances = pd.DataFrame(forest.feature_importances_,\n",
        "                                   index = X_train.columns,\n",
        "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
        "print('\\n',feature_importances.head(10))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
              "           max_features='auto', max_leaf_nodes=None,\n",
              "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "           min_samples_leaf=1, min_samples_split=2,\n",
              "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
              "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Basic RandomForest Regression train score: 0.8064312326481122\n",
            "Basic RandomForest Regression test score: 0.11841351202218797\n",
            "\n",
            "\n",
            "Basic LinReg Regression train score: 1.0\n",
            "Basic LinReg Regression test score: -1697.1970900038077\n",
            "\n",
            "                                         importance\n",
            "capesize_pct1                             0.349743\n",
            "ma5_('lon', 'mean')                       0.120646\n",
            "ma5_('lon', 'median')                     0.048977\n",
            "ma5_('draft', 'mean')                     0.034593\n",
            "ma10_('lon', 'median')                    0.024235\n",
            "pct1_('hours_until_ETA', 'median')        0.021969\n",
            "pct4_status_Under way sailing             0.016222\n",
            "pct9_('draft', 'std')                     0.015718\n",
            "ma10_status_Restricted maneuverability    0.015170\n",
            "pct1_status_Restricted maneuverability    0.013231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cc0PByMF2njV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Selection & Scaling\n",
        "\n",
        "- I face an issue of low obs: only **m=102** days of the Capesize index\n",
        "- With the moving averages and pct_changes included, I have **n=188** features\n",
        "- Therefore I need to use feature selection to extract only the most relevant features\n",
        "- However I also should train my model on a higher m dataset (Capesize index over a longer period)"
      ]
    }
  ]
}