{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Net for S&P500 prediction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/richard-cartwright/personal/blob/master/Neural_Net_for_S&P500_prediction.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "zWrnBUL41er8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This script takes minute-by-minute S&P500 data and predicts the price change for the proceeding minute.\n",
        "# This is only a simplistic first attempt so shows zero optimisation. Notably, I have not used Cross Validation to tune hyperparameters.\n",
        "# I have also done only very basic feature engineering as this is a first attempt.\n",
        "# Features: 10min Moving Averages, and their pct changes, for price and volume.\n",
        "# Target: Pct change of S&P500 in the proceeding minute.\n",
        "# I use out-of-the-box Linreg, Random Forest, CNN - Random Forest is pretty effective w/ a test R^2 of 0.15\n",
        "\n",
        "# 1) Basic imports, including ML libraries\n",
        "# 2) Interacting with Kaggle API\n",
        "# 3) Download Kaggle dataset\n",
        "# 4) Read in data\n",
        "# 5) Create S&P500 Index\n",
        "# 6) Create Moving Averages\n",
        "# 7) Create new df of only features & targets\n",
        "# 8) Add in pct changes & define target variable\n",
        "# 9) Split into train-test and scale features\n",
        "# 10) Linear Regression\n",
        "# 11) Random Forest\n",
        "# 12) Basic CNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQmlSdyU4PHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1) Basic imports, including ML libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# Setting plotting styles\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set_style('white')\n",
        "\n",
        "# Displays all cell's output, not just last output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.preprocessing import StandardScaler, normalize, Normalizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Tensorflow & Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rxwy6JosXEu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 2) Interacting with Kaggle API\n",
        "# From: https://medium.com/@move37timm/using-kaggle-api-for-google-colaboratory-d18645f93648\n",
        "\n",
        "!pip install kaggle\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2qm49Kqs_yv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 3) Download Kaggle dataset\n",
        "\n",
        "# !kaggle datasets list -s 'S&P'\n",
        "!kaggle datasets download nickdl/snp-500-intraday-data\n",
        "!unzip snp-500-intraday-data.zip\n",
        "!unzip dataset.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yotlMoy31s0W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 4) Read in csv with correct index & columns\n",
        "# skipfooter to reduce size of dataset\n",
        "data = pd.read_csv('dataset.csv', header=[0,1], index_col=0, parse_dates=True, infer_datetime_format=True, skipfooter=30000)\n",
        "\n",
        "# Sense check\n",
        "data.head(2)\n",
        "data.info()\n",
        "\n",
        "# Impute missing data via backfilling\n",
        "data.bfill(axis=0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZWAjLK6Crp-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 5) Create S&P500 index by multiplying 'close' and 'volume' at each time period\n",
        "\n",
        "# Create dfs for just 'close' and 'volume' columns\n",
        "close_df = data.loc[:,(slice(None), 'close')]\n",
        "volume_df = data.loc[:,(slice(None), 'volume')]\n",
        "\n",
        "# Drop tier 2 column index\n",
        "close_df.columns = close_df.columns.droplevel(level=1)\n",
        "volume_df.columns = volume_df.columns.droplevel(level=1)\n",
        "\n",
        "# Add new column of S&P500 index to dataframe\n",
        "S&P_index = close_df*volume_df\n",
        "data['S&P500'] = S&P_index.sum(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pSZR29WFNZxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 6) Feature Engineering: create Moving Averages\n",
        "\n",
        "# Create column names with relevant suffix\n",
        "columns_close_MA10 = []\n",
        "columns_volume_MA10 = []\n",
        "for col in close_df.columns:\n",
        "  columns_close_MA10.append(col+'_close_MA10')\n",
        "  columns_volume_MA10.append(col+'_volume_MA10')\n",
        "\n",
        "# Create Noving Average for last 10mins for each of the tickers\n",
        "# MAs for both close(price) & volume\n",
        "close_MA10_df = close_df.rolling(window=10).mean()\n",
        "volume_MA10_df = volume_df.rolling(window=10).mean()\n",
        "\n",
        "# Rename columns with suffix\n",
        "close_MA10_df.columns = columns_close_MA10\n",
        "volume_MA10_df.columns = columns_volume_MA10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mwT5y1VNM1rN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 7) Create new df of only features & target\n",
        "\n",
        "# Create df of only index value\n",
        "model_df = data.loc[:,['S&P500']].copy()\n",
        "model_df.columns = model_df.columns.droplevel(level=1)\n",
        "\n",
        "# Create 10min Moving Average of S&P500\n",
        "model_df['S&P500_MA10'] = model_df['S&P500'].rolling(window=10).mean()\n",
        "\n",
        "# For each time period, for each ticker, add in 10-period MAs for each of 'close' and 'volume'\n",
        "model_df = pd.concat([model_df,close_MA10_df,volume_MA10_df], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XZkUw-S42D9z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 8) Add in pct changes & define target variable\n",
        "\n",
        "# Create column names with relevant suffix\n",
        "columns_change = []\n",
        "for col in model_df.columns:\n",
        "  columns_change.append(col+'_change')\n",
        "\n",
        "# Create one-period pct change for all variables\n",
        "change_df = model_df.pct_change()\n",
        "change_df.columns = columns_change\n",
        "\n",
        "# Concat pct changes to absolute values df\n",
        "model_df = pd.concat([model_df,change_df], axis=1)\n",
        "\n",
        "# This is the predictor variable\n",
        "# Ensure for each time period, we're predicting the next change in price using this period's data \n",
        "model_df['S&P500_change'] = model_df['S&P500_change'].shift(-1)\n",
        "\n",
        "# Drop the (very few) NAs caused by windows & pct changes\n",
        "model_df.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0MWEL0DcUxCc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 9) Split into train-test and scale features\n",
        "\n",
        "# Separate data into predictive and predictor variables\n",
        "X = model_df.drop(['S&P500_change', 'S&P500'], axis=1)\n",
        "y = model_df['S&P500_change']\n",
        "\n",
        "# Define size of train vs test set\n",
        "train_size = 0.7\n",
        "train_num = round(train_size*len(X))\n",
        "\n",
        "# Time-based separation so can't use train-test split\n",
        "# Test set must be after train set to avoid temporal leakage\n",
        "X_train = X.iloc[:train_num]\n",
        "X_test = X.iloc[train_num:]\n",
        "y_train = y[:train_num]\n",
        "y_test = y[train_num:]\n",
        "\n",
        "# Scale features to ensure they're of comparable magnitude\n",
        "# Necessary for some algos like KNN, hugely speeds up other algos using grad descent\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OZS2fCASU5u2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 10) Linear regression to get the party started. Lasso for L1 regularisation\n",
        "# Not optimised regularisation param b/c not done cross validation for the moment\n",
        "\n",
        "lasso = Lasso(alpha=0.01, random_state=42)\n",
        "\n",
        "lasso.fit(X_train, y_train)\n",
        "print('\\n')\n",
        "print('Linreg train R^2: ',lasso.score(X_train, y_train))\n",
        "print('Linreg test R^2: ',lasso.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cH6-jCaKWfoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 11) Random Forest for impressive out-the-box non-linear regression\n",
        "# Not optimised regularisation params b/c not done validation for the moment\n",
        "\n",
        "rfr = RandomForestRegressor( n_estimators=100, \n",
        "                             max_features=10, \n",
        "                             max_depth=10, \n",
        "                             min_samples_split=10, \n",
        "                             min_samples_leaf=10,\n",
        "                             max_leaf_nodes=10,\n",
        "                             n_jobs=-1,\n",
        "                             random_state=42)\n",
        "\n",
        "rfr.fit(X_train, y_train)\n",
        "print('Random Forest train R^2: ',rfr.score(X_train, y_train))\n",
        "print('Random Forest test R^2: ',rfr.score(X_test, y_test))\n",
        "\n",
        "# Get feature importances from our random forest model\n",
        "importances = rfr.feature_importances_\n",
        "\n",
        "# Decide how many top features to viz\n",
        "top_features = 20\n",
        "\n",
        "# Get the index of importances from greatest importance to least\n",
        "sorted_index = np.argsort(importances)[::-1][:top_features]\n",
        "nums = range(top_features)\n",
        "\n",
        "# Use actual feature names  \n",
        "labels = np.array(X.columns)[sorted_index]\n",
        "\n",
        "# Plot top feature importances\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(nums, importances[sorted_index], tick_label=labels)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importances')\n",
        "plt.title('Top predictive features from Random Forest')\n",
        "\n",
        "# Rotate tick labels to vertical\n",
        "plt.xticks(rotation=90)\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gqbSObCm2Eq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 12) Basic Keras Sequential NN with 100-20-1 structure, only 10 epochs for speed\n",
        "# Not optimised any hyperparams (including model structure) b/c not done validation for the moment\n",
        "\n",
        "reg_param = 0.0001\n",
        "\n",
        "basic_nn = Sequential()\n",
        "basic_nn.add(Dense(100, input_dim=X.shape[1], activation='relu', kernel_regularizer=keras.regularizers.l1(reg_param)))\n",
        "basic_nn.add(Dense(20, activation='relu', kernel_regularizer=keras.regularizers.l1(reg_param)))\n",
        "\n",
        "# Linear activation on final node for regression\n",
        "basic_nn.add(Dense(1, activation='linear', kernel_regularizer=keras.regularizers.l1(reg_param)))\n",
        "\n",
        "# Fit the model\n",
        "basic_nn.compile(optimizer='adam', loss='mse')\n",
        "history = basic_nn.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "\n",
        "# Plot the losses from the fit\n",
        "plt.plot(history.history['loss'])\n",
        "\n",
        "# Use the last loss as the title\n",
        "plt.title('basic NN loss:' + str(round(history.history['loss'][-1], 4)))\n",
        "plt.show();\n",
        "\n",
        "# Calculate R^2 score\n",
        "train_preds = basic_nn.predict(X_train)\n",
        "test_preds = basic_nn.predict(X_test)\n",
        "print('Random Forest train R^2: ',r2_score(y_train, train_preds))\n",
        "print('Random Forest test R^2: ',r2_score(y_test, test_preds))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}