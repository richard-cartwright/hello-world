{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PowerCo_churn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "I5ERs_ewhO9k",
        "jhSfj59AhfhM",
        "uzAesX5stqL_",
        "C5Sz2yscnn4F",
        "2eUiNT8Anqay",
        "c1lPEO8ztvZk",
        "PW7H2Uy_ZS8v",
        "9sSlDR_npiaV",
        "x7dtJQaZt2jY",
        "-JsZMGzltTCA",
        "plNlrsXgthwo",
        "nlXbruol3ZVx",
        "DRts6jzUF435",
        "OsadLGyS5Q0V",
        "8TjVKnpLt-IN",
        "h2rCtpkdstYo",
        "_ZsnAlN6Zlcu",
        "nqWJ8UxcB9-p",
        "mDOMVjtcmTiB",
        "fS_vIvBG57ad",
        "a2Io6zql8VRH",
        "6U7Rcpb_0Fbu",
        "y05tN7gu54OI",
        "b1gyXk8h5QLq",
        "1rQaIH8ax_xT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/richard-cartwright/personal/blob/master/PowerCo_churn.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "I5ERs_ewhO9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SUMMARY\n",
        "\n",
        "This is for a consulting case study. The context is that I am working with an imaginary power company. They are concerned by higher-than-industry churn, and are interested in using their customer data to predict which customers will churn.\n",
        "\n",
        "This is meant as a first exploratory meeting with the client, so they are interested in the feasibility of a predictive model. My task is therefore to first explore the data, test the feasibility of a model, and create a first attempt at a model.\n",
        "\n",
        "My presentation slides are here:\n",
        "https://docs.google.com/presentation/d/1jsCm4DJdLRK5zYzfL2Fk4cds_YRX_zDoqrJt6XbOMyE/edit?usp=sharing"
      ]
    },
    {
      "metadata": {
        "id": "kgpfEpkZ5qrn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Story\n",
        "\n",
        "1) The data presented is messy. I therefore spend much time 'sharpening my axe': smartly imputing missing data, creating dummy variables etc. I question the accuracy of the data itself\n",
        "\n",
        "2) This work is exploratory. I therefore do some basic data visualisation, highlighting the most important features for prediction.\n",
        "\n",
        "3) While modelling, I always start with out-of-the-box basic prediction to set a baseline. I then scale my data, interact features to create more complex features, then use feature selection to reduce the size of my input data to make my models run quicker.\n",
        "\n",
        "4) I tune hyperparameters for KNN, then do the same for Random Forest. I have to deal with class imbalance by upsampling as only ~10% customers churn.\n",
        "\n",
        "5) I test my tuned model using my hold-out test data to set an expectation of accuracy based on unseen data. Finally, I train a model (with tuned hyperparameters) using all labelled data, and use this model on the unlabelled submission data."
      ]
    },
    {
      "metadata": {
        "id": "jhSfj59AhfhM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CONTENTS\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Fkm8xU9FkxpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 1) PACKAGES\n",
        "#   a) Installs\n",
        "#   b) Imports\n",
        "\n",
        "# 2) ENVIRON SET-UP\n",
        "\n",
        "# 3) DATA CLEANING & FEATURE ENGINEERING\n",
        "#   a) Merge Train & Test\n",
        "#   b) Model Data\n",
        "#   c) 'Date' Features\n",
        "#   d) 'Forecast' Features\n",
        "#   e) 'Cons' Features\n",
        "#   f) Ordinal Features\n",
        "#   g) Categorical Features\n",
        "#   h) Other Features\n",
        "#   i) Separating into Model & Submission\n",
        "  \n",
        "# 4) DATA VIZ\n",
        "\n",
        "# 5) MODELLING\n",
        "#   a) Train-Val-Test Split\n",
        "#   b) Basic Models\n",
        "#   c) Feature Scaling & Selection\n",
        "#   d) K-Nearest Neighbors\n",
        "#   e) Random Forest\n",
        "#   f) Class Imbalance\n",
        "  \n",
        "# 6) SUBMISSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uzAesX5stqL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PACKAGES"
      ]
    },
    {
      "metadata": {
        "id": "C5Sz2yscnn4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ]
    },
    {
      "metadata": {
        "id": "kZkzezqanfoZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install to visualise ROC curve\n",
        "!pip install scikit-plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eUiNT8Anqay",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "mQmlSdyU4PHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Basic imports, including ML libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pprint\n",
        "%matplotlib inline\n",
        "\n",
        "# Setting plotting styles\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set_style('white')\n",
        "\n",
        "# Displays all cell's output, not just last output\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# Sklearn\n",
        "import scikitplot as skplt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, recall_score\n",
        "\n",
        "# Tensorflow & Keras\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1lPEO8ztvZk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ENVIRON SET-UP"
      ]
    },
    {
      "metadata": {
        "id": "qdFhppw3IARx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add GDrive to Colab environment\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create path for data\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Personal/BCG Gamma/ml_case_data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2RvFMbuTJBcJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract data\n",
        "\n",
        "# Training data\n",
        "training_data = pd.read_csv(path+'/training_data.csv', \n",
        "                        index_col='id', \n",
        "                        parse_dates=['date_activ',\n",
        "                                     'date_end', \n",
        "                                     'date_first_activ',\n",
        "                                     'date_modif_prod', \n",
        "                                     'date_renewal'])\n",
        "training_hist_data = pd.read_csv(path+'/training_hist_data.csv',\n",
        "                             index_col=['id','price_date'], \n",
        "                             parse_dates=['price_date'])\n",
        "training_output = pd.read_csv(path+'/training_output.csv', \n",
        "                              index_col='id')\n",
        "\n",
        "# Submission data\n",
        "test_data = pd.read_csv(path+'/test_data.csv', \n",
        "                        index_col='id', \n",
        "                        parse_dates=['date_activ',\n",
        "                                     'date_end', \n",
        "                                     'date_first_activ',\n",
        "                                     'date_modif_prod', \n",
        "                                     'date_renewal'])\n",
        "test_hist_data = pd.read_csv(path+'/test_hist_data.csv',\n",
        "                             index_col=['id','price_date'], \n",
        "                             parse_dates=['price_date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PW7H2Uy_ZS8v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DATA CLEANING & FEATURE ENGINEERING"
      ]
    },
    {
      "metadata": {
        "id": "9sSlDR_npiaV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Merge Train & Test"
      ]
    },
    {
      "metadata": {
        "id": "Kpj25Nt_phZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Merge model & submission datasets so they go through the same data preprocessing\n",
        "\n",
        "# Create labels for model & submission datasets\n",
        "training_data['train_or_test'] = 'train'\n",
        "test_data['train_or_test'] = 'test'\n",
        "\n",
        "# Concat model & submission datasets\n",
        "whole_data = pd.concat([training_data,test_data])\n",
        "whole_hist_data = pd.concat([training_hist_data,test_hist_data])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7dtJQaZt2jY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Data"
      ]
    },
    {
      "metadata": {
        "id": "WK4UhUwT80vL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Price data for each customer(level 0), for each month of 2015 (level 1)\n",
        "# There are six separate price measures (features)\n",
        "\n",
        "# Calculates proportion of missing data for each customer for each of the six price measures\n",
        "missing_prices = whole_hist_data.isnull().groupby(level=0).mean()\n",
        "missing_prices.columns = ['missing_'+col for col in missing_prices.columns]\n",
        "\n",
        "# Calculates mean for each of the six price measures for each customer\n",
        "mean_prices = whole_hist_data.groupby(level=0).mean()\n",
        "mean_prices.columns = ['mean_'+col for col in mean_prices.columns]\n",
        "\n",
        "# Calculates range (max-min) for each of the six price measures for each customer\n",
        "# Captures whether customer has seen a price change\n",
        "range_prices = whole_hist_data.groupby(level=0).max() - whole_hist_data.groupby(level=0).min()\n",
        "range_prices.columns = ['range_'+col for col in range_prices.columns]\n",
        "\n",
        "# Calculates change (last-first) for each of the six price measures for each customer\n",
        "# Captures size & direction of price change for a customer\n",
        "change_prices = whole_hist_data.groupby(level=0).last() - whole_hist_data.groupby(level=0).first()\n",
        "change_prices.columns = ['change_'+col for col in change_prices.columns]\n",
        "\n",
        "# Concat to create df of all price features\n",
        "whole_prices = pd.concat([missing_prices,mean_prices,range_prices,change_prices], axis=1)\n",
        "\n",
        "# Use median to fillnas\n",
        "whole_prices = whole_prices.fillna(whole_prices.median())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r1kPa9Fh2elT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up model data & target variable\n",
        "\n",
        "# Concat indiv data & price data\n",
        "model_data = pd.concat([whole_data, whole_prices], axis=1).sort_index()\n",
        "\n",
        "# Churn (True or False) is the target variable\n",
        "target = training_output['churn'].sort_index()\n",
        "\n",
        "model_data.info()\n",
        "\n",
        "# Zero data points for 'campaign_disc_ele', therefore drop\n",
        "model_data.drop('campaign_disc_ele', axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JsZMGzltTCA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 'Date' Features"
      ]
    },
    {
      "metadata": {
        "id": "RGEas-Xgqj2V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract 'date' columns\n",
        "\n",
        "date_cols = [col for col in model_data.columns if col[:4]=='date']\n",
        "\n",
        "model_data[date_cols].info()\n",
        "model_data[date_cols].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p7ODgVfhs4kF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'date_end'\n",
        "# Only two missing, and no real pattern - therefore make missing values the median\n",
        "\n",
        "feature = 'date_end'\n",
        "print('#nulls out of 20120 for {}:'.format(feature),sum(model_data[feature].isnull()))\n",
        "\n",
        "# Create bool column if feature is missing \n",
        "model_data['missing_'+feature] = model_data[feature].isnull()\n",
        "\n",
        "# Fill missing values with feature median\n",
        "median_date_end = sorted(model_data[feature])[len(model_data[feature])//2]\n",
        "model_data[feature] = model_data[feature].fillna(median_date_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uRRwiCPmP0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'date_renewal'\n",
        "# date_renewal always exactly one year before date_end (more precisely, from 364-362 days before)\n",
        "\n",
        "feature = 'date_renewal'\n",
        "print('#nulls out of 20120 for {}:'.format(feature),sum(model_data[feature].isnull()))\n",
        "\n",
        "# Create bool column if feature is missing \n",
        "model_data['missing_'+feature] = model_data[feature].isnull()\n",
        "\n",
        "# Fill missing values with 'date_end' minus 364 days\n",
        "model_data[feature] = model_data[feature].fillna(model_data['date_end'] - pd.DateOffset(years=1,days=-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ff610sRtowBX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'date_modif_prod'\n",
        "# For the vast majority of 'date_modif_prod', it is exactly the same as 'date_activ'\n",
        "# Therefore, set missing 'date_modif_prod'='date_activ'\n",
        "\n",
        "feature = 'date_modif_prod'\n",
        "print('#nulls out of 20120 for {}:'.format(feature),sum(model_data[feature].isnull()))\n",
        "\n",
        "# Create bool column if feature is missing \n",
        "model_data['missing_'+feature] = model_data[feature].isnull()\n",
        "\n",
        "# Fill missing values with 'date_activ'\n",
        "model_data[feature] = model_data[feature].fillna(model_data['date_activ'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dg9CrJjusGPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# date_first_activ\n",
        "# For the non-missing 'date_first_activ', they are exactly the same as 'date_activ'\n",
        "# Therefore, set missing 'date_first_activ'='date_activ'\n",
        "\n",
        "feature = 'date_first_activ'\n",
        "print('#nulls out of 20120 for {}:'.format(feature),sum(model_data[feature].isnull()))\n",
        "\n",
        "# Create bool column if feature is missing \n",
        "model_data['missing_'+feature] = model_data[feature].isnull()\n",
        "\n",
        "# Fill missing values with 'date_activ'\n",
        "model_data[feature] = model_data[feature].fillna(model_data['date_activ'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5KyhuromDc3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For each of the five date columns, extract 'year' and 'month'\n",
        "# Year is ordinal so leave as one feature, whereas Quarter is categorical so create dummy vars\n",
        "\n",
        "quarter_date_cols = []\n",
        "# For each date col, create Year & Quarter features\n",
        "for col in date_cols:\n",
        "  model_data['quarter_'+col] = model_data[col].dt.quarter\n",
        "  model_data['year_'+col] = model_data[col].dt.year\n",
        "  quarter_date_cols.extend(['quarter_'+col])\n",
        "\n",
        "# Drop original datetime cols\n",
        "model_data = model_data.drop(date_cols,axis=1)\n",
        "\n",
        "# Create dummy variables for Quarter features\n",
        "model_data = pd.get_dummies(model_data, columns=quarter_date_cols, drop_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "plNlrsXgthwo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 'Forecast' Features"
      ]
    },
    {
      "metadata": {
        "id": "HiYsPX0ts_Jz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract 'forecast' columns\n",
        "\n",
        "forecast_cols = [col for col in model_data.columns if col[:8]=='forecast']\n",
        "\n",
        "model_data[forecast_cols].info()\n",
        "model_data[forecast_cols].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ytkt4wtuvp4g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loads of these columns have excessive zeros - could be a data inaccuracy.\n",
        "# Therefore, create extra bool feature if value = zero, and then set value to feature median. \n",
        "\n",
        "for col in forecast_cols:\n",
        "  feature = col\n",
        "  print('#nulls out of 20120 for {}:'.format(feature),sum(model_data[feature].isnull()))\n",
        "  print('#zeros out of 20120 for {}:'.format(feature),sum((model_data[feature]==0)))\n",
        "\n",
        "  # Create bool column if feature is missing or zero\n",
        "  model_data['missing_'+feature] = model_data[feature].isnull()\n",
        "  model_data['zero_'+feature] = (model_data[feature]==0)\n",
        "  \n",
        "  # Replace missings and zeros with feature median\n",
        "  col_median = model_data[feature].median()\n",
        "  model_data[feature] = model_data[feature].replace(to_replace=0, value=col_median)\n",
        "  model_data[feature] = model_data[feature].fillna(col_median)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nlXbruol3ZVx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 'Cons' Features"
      ]
    },
    {
      "metadata": {
        "id": "vQC7eJoH3YeY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract 'cons' columns\n",
        "\n",
        "cons_cols = [col for col in model_data.columns if col[:4]=='cons']\n",
        "\n",
        "model_data[cons_cols].info()\n",
        "model_data[cons_cols].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Slvvn7Dd4IOW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loads of these columns have excessive zeros - could be a data inaccuracy.\n",
        "# Therefore, create extra bool feature if value = zero, and then set value to feature median. \n",
        "\n",
        "for col in cons_cols:\n",
        "  feature = col\n",
        "  print('#nulls out of 20120 for {}:'.format(feature),sum(model_data[feature].isnull()))\n",
        "  print('#zeros out of 20120 for {}:'.format(feature),sum((model_data[feature]==0)))\n",
        "\n",
        "  # Create bool column if feature is missing or zero\n",
        "  model_data['missing_'+feature] = model_data[feature].isnull()\n",
        "  model_data['zero_'+feature] = (model_data[feature]==0)\n",
        "  \n",
        "  # Replace missings and zeros with feature median\n",
        "  col_median = model_data[feature].median()\n",
        "  model_data[feature] = model_data[feature].replace(to_replace=0, value=col_median)\n",
        "  model_data[feature] = model_data[feature].fillna(col_median)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DRts6jzUF435",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ordinal Features"
      ]
    },
    {
      "metadata": {
        "id": "-RY0QDe_F4Rl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract Ordinal columns\n",
        "\n",
        "ordinal_cols = ['nb_prod_act', 'num_years_antig']\n",
        "\n",
        "model_data[ordinal_cols].info()\n",
        "model_data[ordinal_cols].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R5MNxm7Zuvoh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Planned to restrict the range of the data so I could make dummy variables.\n",
        "# But in the end left as Ordinal variables with data untouched."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lvnuPwuoGeI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'nb_prod_act'\n",
        "\n",
        "# model_data['nb_prod_act'].groupby(model_data['nb_prod_act']).count()\n",
        "# model_data.loc[model_data['nb_prod_act'] >=4, 'nb_prod_act'] = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WC3twOGVLEcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'num_years_antig'\n",
        "\n",
        "# model_data['num_years_antig'].groupby(model_data['num_years_antig']).count()\n",
        "# model_data.loc[model_data['num_years_antig'] <=3, 'num_years_antig'] = 3\n",
        "# model_data.loc[model_data['num_years_antig'] >=12, 'num_years_antig'] = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsadLGyS5Q0V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Categorical Features"
      ]
    },
    {
      "metadata": {
        "id": "netMs6cz5QOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract Categorical columns\n",
        "\n",
        "categorical_cols = ['activity_new', 'channel_sales', 'origin_up']\n",
        "\n",
        "model_data[categorical_cols].info()\n",
        "model_data[categorical_cols].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q7EqXF4x5t4Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If data is missing for a feature, fill with 'missing'\n",
        "# If the data is part of a category with <=100 hits, replace with 'too_low'.\n",
        "# Do this so have a reasonable (lower) amount of dummy variables.\n",
        "\n",
        "for col in categorical_cols:\n",
        "  feature = col\n",
        "\n",
        "  # Fill missing with 'missing'\n",
        "  model_data[feature] = model_data[feature].fillna('missing')\n",
        "\n",
        "  # Count of hits for each category for the feature\n",
        "  col_distn = model_data[feature].groupby(model_data[feature]).count().sort_values(ascending=False)\n",
        "  \n",
        "  # Replace categories with <=100 hits with 'too_low' - reduces the amount of dummy vars\n",
        "  feature_replace = list(set(col_distn.index) - set(col_distn[col_distn.where(col_distn>100).notnull()].index))\n",
        "  model_data[feature] = model_data[feature].replace(to_replace=feature_replace, value='too_low')\n",
        "  \n",
        "  # Cuts feature name from long hash to first 7 characters\n",
        "  model_data[feature] = model_data[feature].apply(lambda x: x[:7])\n",
        "  \n",
        "# Create dummy vars for the categorical vars\n",
        "model_data = pd.get_dummies(model_data, columns=categorical_cols, drop_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X14SKaQZDTqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 'has_gas'\n",
        "# Originally, 't' is True and 'f' is False. This turns it into a bool column.\n",
        "\n",
        "model_data['has_gas'] = (model_data['has_gas'] == 't')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8TjVKnpLt-IN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Other Features"
      ]
    },
    {
      "metadata": {
        "id": "1HSE3oHm0wY6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract remaining (numerical) columns\n",
        "\n",
        "other_cols = ['imp_cons',\n",
        "              'margin_gross_pow_ele',\n",
        "              'margin_net_pow_ele',\n",
        "              'net_margin',\n",
        "              'pow_max']\n",
        "\n",
        "model_data[other_cols].info()\n",
        "model_data[other_cols].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hB4ohvnBM0xQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loads of these columns have excessive zeros - could be a data inaccuracy.\n",
        "# Therefore, create extra bool feature if value = zero, and then set value to feature median. \n",
        "\n",
        "for col in other_cols:\n",
        "  feature = col\n",
        "  print('#nulls out of 20120 for {}:'.format(feature),sum(model_data[feature].isnull()))\n",
        "  print('#zeros out of 20120 for {}:'.format(feature),sum((model_data[feature]==0)))\n",
        "\n",
        "   # Create bool column if feature is missing or zero\n",
        "  model_data['missing_'+feature] = model_data[feature].isnull()\n",
        "  model_data['zero_'+feature] = (model_data[feature]==0)\n",
        "  \n",
        "  # Replace missings and zeros with feature median\n",
        "  col_median = model_data[feature].median()\n",
        "  model_data[feature] = model_data[feature].replace(to_replace=0, value=col_median)\n",
        "  model_data[feature] = model_data[feature].fillna(col_median)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2rCtpkdstYo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Separating into Model & Submission"
      ]
    },
    {
      "metadata": {
        "id": "Zjx5ZYD_s3Vq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reseparate data into Model & Submission data\n",
        "\n",
        "model_data_train = model_data[model_data.train_or_test == 'train'].drop('train_or_test',axis=1)\n",
        "model_data_submission = model_data[model_data.train_or_test == 'test'].drop('train_or_test',axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZsnAlN6Zlcu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DATA VIZ"
      ]
    },
    {
      "metadata": {
        "id": "ZE007VSJ3cZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get initial feature importances for visualisation\n",
        "\n",
        "# First scale data as better for viz\n",
        "scaled_data = pd.DataFrame(data=StandardScaler().fit_transform(model_data_train), \n",
        "                           index=model_data_train.index, \n",
        "                           columns=model_data_train.columns)\n",
        "\n",
        "# Use out-of-box RandomForest for feature importances\n",
        "rfc = RandomForestClassifier()\n",
        "RandomForestClassifier().fit(scaled_data, target)\n",
        "feature_importances = pd.DataFrame(rfc.feature_importances_,\n",
        "                                   index = scaled_data.columns,\n",
        "                                   columns=['importance']).sort_values('importance',ascending=False)\n",
        "\n",
        "# Print 10 most important features\n",
        "print(feature_importances.head(10),'\\n')\n",
        "\n",
        "# Extract & visualise via boxplots the 10 most important features\n",
        "features = list(feature_importances.index)[:10]\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(scaled_data[features], \n",
        "            showfliers=False, \n",
        "            vert=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bWBgvwbERnd8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Usa PCA to visualise data in 2D\n",
        "\n",
        "pca_2d = PCA(n_components=2,random_state=42)\n",
        "twodim_pca_data = pd.DataFrame(data=pca_2d.fit_transform(scaled_data),\n",
        "                               index=scaled_data.index,\n",
        "                               columns=['x1','x2'])\n",
        "\n",
        "# Plot 2 dimensions, with whether 'churn'==1 dictating the point's colour\n",
        "two_dim_viz_df = pd.concat([twodim_pca_data,target], axis=1)\n",
        "two_dim_viz_df.plot.scatter('x1','x2',c='churn',alpha=0.2,cmap='RdYlGn');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oi3jXxfdVvYQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualising difference in churn based on when the customer first became active\n",
        "\n",
        "# For each year, get mean proportion who churned\n",
        "viz_data = pd.concat([model_data_train,target], axis=1).groupby('year_date_first_activ')['churn'].mean()\n",
        "\n",
        "# Visualise only 2008 to 2013\n",
        "viz_data = viz_data.loc[2008:2013]\n",
        "viz_data.plot(kind='bar',\n",
        "              title='Proportion churn by year of date of first contract',\n",
        "              rot=0);\n",
        "plt.ylabel('Proportion who churn \\n in a given year');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nqWJ8UxcB9-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MODELLING"
      ]
    },
    {
      "metadata": {
        "id": "mDOMVjtcmTiB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train-Val-Test Split"
      ]
    },
    {
      "metadata": {
        "id": "bZW9-Fx12DHG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split data into Train-Val-Test (60-20-20)\n",
        "\n",
        "# train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(model_data_train, \n",
        "                                                    target, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42)\n",
        "\n",
        "# withold 25% of training set for validation of hyperparameters\n",
        "validation_threshold = round(len(X_train)*3/4)\n",
        "X_val = X_train.iloc[validation_threshold:]\n",
        "y_val = y_train[validation_threshold:]\n",
        "\n",
        "# make X_train & y_train exclude validation data\n",
        "X_train = X_train.iloc[:validation_threshold]\n",
        "y_train = y_train[:validation_threshold]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fS_vIvBG57ad",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic Models"
      ]
    },
    {
      "metadata": {
        "id": "RTfqpHaeu6SK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LogReg out-of-the-box to get a baseline - plot ROCs for training & test\n",
        "\n",
        "# no optimisation, just to see basic accuracy\n",
        "logreg = LogisticRegression(random_state=42)\n",
        "logreg.fit(X_train,y_train)\n",
        "\n",
        "probs_train = logreg.predict_proba(X_train)\n",
        "probs_test = logreg.predict_proba(X_test)\n",
        "\n",
        "print('Basic logreg training roc_auc_score:', roc_auc_score(y_train,probs_train[:,1]))\n",
        "print('Basic logreg test roc_auc_score:', roc_auc_score(y_test,probs_test[:,1]))\n",
        "\n",
        "# Plot ROCs for Train & Test\n",
        "skplt.metrics.plot_roc(y_train,probs_train, title='LogReg Train ROC');\n",
        "skplt.metrics.plot_roc(y_test,probs_test, title='LogReg Test ROC');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "laAmwsae5_YH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# RandomForest out-of-the-box to get a baseline - plot ROCs for training & test\n",
        "\n",
        "# no optimisation, just to see basic accuracy\n",
        "forest = RandomForestClassifier(random_state=42,\n",
        "                                n_estimators=100,\n",
        "                                max_depth=10)\n",
        "forest.fit(X_train,y_train)\n",
        "\n",
        "probs_train = forest.predict_proba(X_train)\n",
        "probs_test = forest.predict_proba(X_test)\n",
        "\n",
        "print('Basic RandomForest training roc_auc_score:', roc_auc_score(y_train,probs_train[:,1]))\n",
        "print('Basic RandomForest test roc_auc_score:', roc_auc_score(y_test,probs_test[:,1]))\n",
        "\n",
        "# Plot ROCs for Train & Test\n",
        "skplt.metrics.plot_roc(y_train,probs_train, title='RandomForest Train ROC');\n",
        "skplt.metrics.plot_roc(y_test,probs_test, title='RandomForest Test ROC');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a2Io6zql8VRH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling & Selection"
      ]
    },
    {
      "metadata": {
        "id": "0flbXcIa8w8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create pipeline for Feature Scaling & Selection\n",
        "# First scale features,\n",
        "# then create interactions to poly=2, \n",
        "# then use PCA to reduce to 50 features\n",
        "\n",
        "features_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('polyFeat', PolynomialFeatures(degree=2, \n",
        "                                    include_bias=False)),\n",
        "    ('pca', PCA(n_components=50, \n",
        "                random_state=42))\n",
        "])\n",
        "\n",
        "# Fit then transform Train-Val-Test data\n",
        "X_train = features_pipe.fit_transform(X_train)\n",
        "X_val = features_pipe.transform(X_val)\n",
        "X_test = features_pipe.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6U7Rcpb_0Fbu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors"
      ]
    },
    {
      "metadata": {
        "id": "MMuFkj1NzxqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for KNN\n",
        "\n",
        "# Implementation takes a few minutes, so commented out to avoid accidentally running.\n",
        "\n",
        "# neighbors_params = [1,2,4,7,11,16,22,29,37,46,56,67]\n",
        "# knn_train_scores = []\n",
        "# knn_val_scores = []\n",
        "\n",
        "# for k in neighbors_params:\n",
        "#   knn = KNeighborsClassifier(n_neighbors=k)\n",
        "#   knn.fit(X_train,y_train)\n",
        "#   # print('\\n')\n",
        "  \n",
        "#   probs_train = knn.predict_proba(X_train)\n",
        "#   probs_val = knn.predict_proba(X_val)\n",
        "  \n",
        "#   knn_train_scores.append(roc_auc_score(y_train,probs_train[:,1]))\n",
        "#   knn_val_scores.append(roc_auc_score(y_val,probs_val[:,1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "flOSj2eS6T8H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot Train & Val scores for different K in KNN\n",
        "\n",
        "knn_scores_df = pd.DataFrame(data={'train':knn_train_scores,\n",
        "                                   'val':knn_val_scores},\n",
        "                             index=neighbors_params)\n",
        "knn_scores_df.plot(title='KNN roc_auc_score for different k');\n",
        "plt.xlabel('#neighbors');\n",
        "plt.ylabel('roc_auc_score');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y05tN7gu54OI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "Jf_6JNWM8OOh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for Random Forest, using the two key hyperparameters:\n",
        "# i) Number of estimators: how many trees in forest\n",
        "# ii) Maximum depth: maximum layers of nodes\n",
        "\n",
        "# Implementation takes a few minutes, so commented out to avoid accidentally running.\n",
        "\n",
        "# tree_estimators_params = [int(round(n)) for n in np.logspace(0,3,num=4)]\n",
        "# tree_depth_params = [int(round(depth)) for depth in np.logspace(0.5,2,num=4)]\n",
        "# tree_scores_df = pd.DataFrame(columns=['n_estimators',\n",
        "#                                        'max_depth',\n",
        "#                                        'train_score',\n",
        "#                                        'val_score'])\n",
        "\n",
        "# for n in tree_estimators_params:\n",
        "#   for depth in tree_depth_params:\n",
        "#     forest = RandomForestClassifier(random_state=42,\n",
        "#                                     n_estimators=n,\n",
        "#                                     max_depth=depth)\n",
        "#     forest.fit(X_train,y_train)\n",
        "\n",
        "#     probs_train = forest.predict_proba(X_train)\n",
        "#     probs_val = forest.predict_proba(X_val)\n",
        "    \n",
        "#     tree_scores_df = tree_scores_df.append(\n",
        "#       {'n_estimators':n,\n",
        "#        'max_depth':depth,\n",
        "#        'train_score':roc_auc_score(y_train,probs_train[:,1]),\n",
        "#        'val_score':roc_auc_score(y_val,probs_val[:,1])\n",
        "#       },\n",
        "#       ignore_index=True\n",
        "#     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7MUN0oPfqRN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create heatmap of n_estimators vs max_depth, with heat=validation_score\n",
        "\n",
        "sns.heatmap(tree_scores_df.pivot_table(values='val_score',index='n_estimators',columns='max_depth'),\n",
        "            annot=True);\n",
        "plt.title('Validation score for two key hyperparams \\n for Random Forest');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b1gyXk8h5QLq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Class Imbalance"
      ]
    },
    {
      "metadata": {
        "id": "aKUfB5OlA-KT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Only ~10% of customers churn - therefore problem of class imbalance.\n",
        "# Recall is very low. This means that of those who do actually churn, very few are predicted to do so.\n",
        "\n",
        "# Therefore, I try out different reweightings for churn==1 class, from 1 to 144.\n",
        "\n",
        "# Implementation takes a few minutes, so commented out to avoid accidentally running.\n",
        "\n",
        "# tree_weighting_params = [i**2 for i in range(1,13)]\n",
        "# tree_weighting_scores_df = pd.DataFrame(columns=['weighting',\n",
        "#                                                  'train_score',\n",
        "#                                                  'val_score',\n",
        "#                                                  'val_recall'])\n",
        "\n",
        "# for weight in tree_weighting_params:\n",
        "#   forest = RandomForestClassifier(random_state=42,\n",
        "#                                   n_estimators=100,\n",
        "#                                   max_depth=32,\n",
        "#                                   class_weight={1:weight})\n",
        "#   forest.fit(X_train,y_train)\n",
        "\n",
        "#   probs_train = forest.predict_proba(X_train)\n",
        "#   probs_val = forest.predict_proba(X_val)\n",
        "\n",
        "#   tree_weighting_scores_df = tree_weighting_scores_df.append(\n",
        "#     {'weighting':weight,\n",
        "#      'train_score':roc_auc_score(y_train,probs_train[:,1]),\n",
        "#      'val_score':roc_auc_score(y_val,probs_val[:,1]),\n",
        "#      'val_recall':recall_score(y_val,forest.predict(X_val))\n",
        "#     },\n",
        "#     ignore_index=True\n",
        "#   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XK5qkvaxC8Rv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot the train_score, val_score, and val_recall against the different weightings\n",
        "tree_weighting_scores_df.set_index('weighting').plot(title='Upsampling the churn=1 data to combat \\n class imbalance');\n",
        "plt.xlabel('Weighting on churn=1 class');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1rQaIH8ax_xT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SUBMISSION"
      ]
    },
    {
      "metadata": {
        "id": "iXFjKDGrI0zK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use optimal hyperparameters to fit model, extracting final auc_score on Test data.\n",
        "\n",
        "forest = RandomForestClassifier(random_state=42,\n",
        "                                n_estimators=100,\n",
        "                                max_depth=32,\n",
        "                                class_weight={1:5})\n",
        "\n",
        "forest.fit(X_train,y_train);\n",
        "probs_test = forest.predict_proba(X_test)\n",
        "print('\\n \\n', 'Final test roc_auc_score:', roc_auc_score(y_test,probs_test[:,1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLLv7KPxyBuU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use Random Forest as the model.\n",
        "# Use optimal hyperparameters and entire model dataset (Train-Val-Test) to fit model.\n",
        "# Make predictions (with probabilities) for submission  data.\n",
        "\n",
        "# Model data uses all of labelled data, submission data is without labels\n",
        "X_model = features_pipe.fit_transform(model_data_train)\n",
        "X_submission = features_pipe.transform(model_data_submission)\n",
        "\n",
        "# Random Forest with optimised hyperparameters\n",
        "forest = RandomForestClassifier(random_state=42,\n",
        "                                n_estimators=100,\n",
        "                                max_depth=32,\n",
        "                                class_weight={1:5})\n",
        "forest.fit(X_model,target)\n",
        "\n",
        "# Use fitted model to predict labels and probabilities for submission data\n",
        "submission_df = pd.DataFrame(\n",
        "    data={'Churn_prediction':forest.predict(X_submission),\n",
        "          'Churn_probability':forest.predict_proba(X_submission)[:,1]\n",
        "         },\n",
        "    index=model_data_submission.index\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2rLke9-MNlm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download submission as a csv\n",
        "\n",
        "# submission_df.to_csv('Powerco_submission_RichardCartwright.csv', index=True)\n",
        "# from google.colab import files\n",
        "# files.download('Powerco_submission_RichardCartwright.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
